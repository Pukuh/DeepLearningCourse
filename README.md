# DeepLearningCourse

## 1. Statistical Learning Theory
1. The Learning Problem
2. Hoeffding Inequality
3. Hoeffding Inequality Intuition
4. Uniform Bounds
5. VC-Dimension  

### Readings
1. The Learning Problem (Chapter 1 from [1])
2. Training versus Testing (Chapter 2 from [1])

### Videos
1. El problema de aprendizaje de máquina [Youtube-Lab](https://youtu.be/DIlfHntZPv8) [Slides](https://drive.google.com/file/d/1SJJd7TtqnX_Dkq60fpN8MY5s_R_G37Xm/view?usp=sharing)
2. Teoría de Aprendizaje Estadístico I [Youtube-Lab](https://youtu.be/zUawHEGadAA)
3. Teoría de Aprendizaje Estadístico II [Youtube-Lab](https://youtu.be/3U3XKN0opXA)

### Code Projects


_References:_ 
[1] Learning from data ([MOOC](https://work.caltech.edu/telecourse)). Abu Mostafa 
[2] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. _Learning from Data_. AMLBook, 2012.


## 1.1 Statistical Learning Theory II (Optional)
1. Bound of the Growth Function
2. Proof of the VC-Dimension Bound

### Readings
1. Proof of the Bound of the Growth Function (Chapter 2 from [1])
2. Proof of the VC-Dimension Bound (Appendix from [1])

### Videos

_References:_
[1] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. _Learning from Data_. AMLBook, 2012.

---
## 2. Working with tensors in Pytorch: Broadcasting, view, etc.

_References:_
[1] Pytorch documentation. [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html) 
[2] VanderPlas, Jake. Python data science handbook: Essential tools for working with data. " O'Reilly Media, Inc.", 2016.

---
## 4. Computational Graphs:
- Forward Computation    
- Differentiation Review    

_References:_
[1] Goodfellow, I, Y Bengio, and A Courville. Deep Learning. The MIT Press. The MIT Press, 2017.
[2] Hubbard and Hubbard, Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach.

---
## 5. Computational Graphs II:
- Differentiation Review    
- Backward Computation (BackProp)     

_References:_ 
[1] Goodfellow, I, Y Bengio, and A Courville. Deep Learning. The MIT Press. The MIT Press, 2017. 
[2] Hubbard and Hubbard, Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach.

---
## 6. Convolutional Neural Networks 
- Computation of Convolutions 

_References:_
[1] Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” arXiv:2104.13478 [Cs, Stat], May 2, 2021. [http://arxiv.org/abs/2104.13478](http://arxiv.org/abs/2104.13478).

---
## 7. Skip/Residual/Shortcut-connections 

_References:_ 
[1] [Deep Residual Learning for Image Recognition (He et al, CVPR2016)](https://arxiv.org/pdf/1512.03385v1.pdf) 
[2] [Identity Mappings in Deep Residual Networks (He et al, ECCV2016)](https://arxiv.org/pdf/1603.05027.pdf) 
[3] [Residual Networks (Coursera)](https://www.coursera.org/lecture/convolutional-neural-networks/resnets-HAhz9) 
[4] [Why ResNets work (Coursera)](https://www.coursera.org/lecture/convolutional-neural-networks/why-resnets-work-XAKNO) 
[5] [Dive into Deep Learning Book](https://d2l.ai/)

---
## 8. Dropout 

_References:_ 
[1] Goodfellow, I, Y Bengio, and A Courville. Deep Learning. The MIT Press. The MIT Press, 2017. 
[2] Srivastava, Nitish, and Geoffrey Hinton. “Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting.” Journal of Machine Learning Research, 2014. 
[3] Hinton, Geoffrey E, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.” ArXiv, 2012. 
[4] Gal, Yarin, and Zoubin Ghahramani. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” ArXiv, 2015.

---
## Attention Mechanisms 

_References:_
[1] Vaswani et al., “Attention Is All You Need.”
[2] [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) 
[3] [Murphy, Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html).

---
## Graph Neural Networks

_References:_
[1] Hamilton, William L. “Graph Representation Learning.” Synthesis Lectures on Artifical Intelligence and Machine Learning 14, no. 3 (2020): 1–159.