# DeepLearningCourse

## 1. Statistical Learning Theory

1. The Learning Problem
2. Hoeffding Inequality
3. Hoeffding Inequality Intuition
4. Uniform Bounds
5. VC-Dimension  

### Readings

1. The Learning Problem (Chapter 1 from [2])
2. Training versus Testing (Chapter 2 from [2])

### Videos

1. El problema de aprendizaje de máquina [Youtube-Lab](https://youtu.be/DIlfHntZPv8) [Slides](https://drive.google.com/file/d/1SJJd7TtqnX_Dkq60fpN8MY5s_R_G37Xm/view?usp=sharing)
2. Teoría de Aprendizaje Estadístico I [Youtube-Lab](https://youtu.be/zUawHEGadAA)
3. Teoría de Aprendizaje Estadístico II [Youtube-Lab](https://youtu.be/3U3XKN0opXA)

### Code Projects

_References:_
[1] Learning from data ([MOOC](https://work.caltech.edu/telecourse)). Abu Mostafa 
[2] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. _Learning from Data_. AMLBook, 2012.

## 1.1 Statistical Learning Theory II (Optional)

1. Bound of the Growth Function
2. Proof of the VC-Dimension Bound

### Readings

1. Proof of the Bound of the Growth Function (Chapter 2 from [1])
2. Proof of the VC-Dimension Bound (Appendix from [1])

### Videos

_References:_
[1] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. _Learning from Data_. AMLBook, 2012.

---

## 2. Working with tensors in Pytorch: Broadcasting, view, etc.

### Readings

1. Follow pytorch tensors tutorial [1].
2. Introduction to NumPy (Part II [3])

### Videos

1. Introducción a PyTorch [Youtube-Lab](https://youtu.be/Z-pzCitkGUM) [Slides](https://drive.google.com/file/d/1PLl3L--nQZYoGah3oJ9Ohg1DOl2BChDu/view?usp=sharing)

_References:_
[1] Pytorch [tutorials-Tensors](https://docs.pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html). 
[2] Pytorch documentation. [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html) 
[3] VanderPlas, Jake. Python data science handbook: Essential tools for working with data. " 2ed O'Reilly Media, Inc.", 2023.

## 3.1 Stochastic Gradient Descent

1. Gradient descent
2. Stochastic gradient descent
3. Momentum
4. Adam

### Readings

1. Fitting Models (Chapter 6 [1])[Youtube-Lab](https://youtu.be/jiRW12V1NwA)[Slides](https://udlbook.github.io/udlbook/)
2. Gradient Descent (Chapter 7 [2])

_References:_
[1] Prince, Simon J.D. _Understanding Deep Learning_. The MIT Press, 2023. [http://udlbook.com](http://udlbook.com).
[2] Bishop, Christopher Michael, and Hugh Bishop. _Deep Learning - Foundations and Concepts_. 1st ed. Edited by Springer Cham. 2023. [https://doi.org/10.1007/978-3-031-45468-4](https://doi.org/10.1007/978-3-031-45468-4).

## 4. Computational Graphs:

- Forward Computation
- Differentiation Review    

_References:_
[1] Goodfellow, I, Y Bengio, and A Courville. Deep Learning. The MIT Press. The MIT Press, 2017.
[2] Hubbard and Hubbard, Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach.

---
## 5. Computational Graphs II:
- Differentiation Review    
- Backward Computation (BackProp)     

_References:_ 
[1] Goodfellow, I, Y Bengio, and A Courville. Deep Learning. The MIT Press. The MIT Press, 2017. 
[2] Hubbard and Hubbard, Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach.

---
## 6. Convolutional Neural Networks 
- Computation of Convolutions 

_References:_
[1] Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” arXiv:2104.13478 [Cs, Stat], May 2, 2021. [http://arxiv.org/abs/2104.13478](http://arxiv.org/abs/2104.13478).

---
## 7. Skip/Residual/Shortcut-connections 

_References:_ 
[1] [Deep Residual Learning for Image Recognition (He et al, CVPR2016)](https://arxiv.org/pdf/1512.03385v1.pdf) 
[2] [Identity Mappings in Deep Residual Networks (He et al, ECCV2016)](https://arxiv.org/pdf/1603.05027.pdf) 
[3] [Residual Networks (Coursera)](https://www.coursera.org/lecture/convolutional-neural-networks/resnets-HAhz9) 
[4] [Why ResNets work (Coursera)](https://www.coursera.org/lecture/convolutional-neural-networks/why-resnets-work-XAKNO) 
[5] [Dive into Deep Learning Book](https://d2l.ai/)

---
## 8. Dropout 

_References:_ 
[1] Goodfellow, I, Y Bengio, and A Courville. Deep Learning. The MIT Press. The MIT Press, 2017. 
[2] Srivastava, Nitish, and Geoffrey Hinton. “Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting.” Journal of Machine Learning Research, 2014. 
[3] Hinton, Geoffrey E, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.” ArXiv, 2012. 
[4] Gal, Yarin, and Zoubin Ghahramani. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” ArXiv, 2015.

---
## Attention Mechanisms 

_References:_
[1] Vaswani et al., “Attention Is All You Need.”
[2] [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) 
[3] [Murphy, Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html).

---
## Graph Neural Networks

_References:_
[1] Hamilton, William L. “Graph Representation Learning.” Synthesis Lectures on Artifical Intelligence and Machine Learning 14, no. 3 (2020): 1–159.